---
title: "Overview"
---

## High-level description
This directory contains the API and schema definitions for generating comprehensive data quality reports in the Elementary monitoring system. It includes classes and utilities for fetching, aggregating, and structuring data from various sources within a data pipeline, such as test results, model runs, lineage information, and source freshness.

## What does it do?
The code in this directory is responsible for:

1. Defining the structure of the report data using Pydantic schemas.
2. Providing an API to fetch and aggregate data from various sources within the data pipeline.
3. Calculating and summarizing test results and test run statistics.
4. Organizing all the collected data into a standardized format for easy consumption and presentation in reports.

This functionality allows users to generate comprehensive reports on the health and quality of their data pipelines, including information about test results, model performance, data lineage, and source freshness.

## Key Files

### report.py
This file contains the `ReportAPI` class, which is the main entry point for generating report data. It orchestrates the process of fetching data from various API endpoints, aggregating the information, and compiling it into a structured report format.

Key features:
- Interacts with multiple API classes to retrieve different types of data (e.g., tests, models, lineage).
- Processes and aggregates data from various sources.
- Returns a compiled `ReportDataSchema` object containing all the report data.

Example usage:
```python
report_api = ReportAPI(dbt_runner)
report_data = report_api.get_report_data()
```

### schema.py
This file defines the Pydantic schemas used to structure and validate the report data. The main schema is `ReportDataSchema`, which encompasses all the information needed for a comprehensive data quality report.

Key components:
- `ReportDataSchema`: The central schema that aggregates data from various other schemas.
- `ReportDataEnvSchema`: A schema for environment-related data in the report.

These schemas ensure that the report data is well-structured and type-safe, making it easier to work with and present the information.

### totals_utils.py
This file provides utility functions for calculating aggregated test results and test run statistics. These functions are crucial for summarizing the overall health of the data pipeline.

Key functions:
- `get_total_test_results`: Aggregates test result statuses from a list of test results.
- `get_total_test_runs`: Aggregates test run statuses from a list of test runs.

Example usage:
```python
test_results = {...}  # Dictionary of test results
totals = get_total_test_results(test_results)
```

## Dependencies
The code in this directory relies on several external libraries and internal modules:

- Pydantic: Used for data validation and settings management using Python type annotations.
- typing: Provides support for type hints.
- Various internal Elementary modules, such as:
  - `elementary.monitor.api.tests.tests.TestsAPI`
  - `elementary.monitor.api.source_freshnesses.source_freshnesses.SourceFreshnessesAPI`
  - `elementary.monitor.api.models.models.ModelsAPI`
  - `elementary.monitor.api.lineage.lineage.LineageAPI`
  - `elementary.monitor.api.filters.filters.FiltersAPI`
  - `elementary.monitor.api.invocations.invocations.InvocationsAPI`

These dependencies allow the report generation system to interact with various components of the Elementary monitoring system and ensure type safety and data validation throughout the process.

## Configuration
While there are no explicit configuration files in this directory, the behavior of the report generation can be influenced by the configuration of the underlying API classes and the DBT runner. The `ReportAPI` class takes a `dbt_runner` instance as an input, which may have its own configuration settings that affect how data is fetched and processed.

In summary, this directory provides a crucial part of the Elementary monitoring system by defining the structure and logic for generating comprehensive data quality reports. It aggregates data from various sources within a data pipeline, calculates summary statistics, and presents the information in a well-structured format for easy consumption and analysis.