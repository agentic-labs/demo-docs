---
title: "test_rag.py"
---

## High-level description
This file contains unit tests for the RAG (Retrieval-Augmented Generation) functionality in a machine learning system. It tests various aspects of the RAG implementation, including error handling, different LLM types (OpenAI and Writer), vector stores, and question-answering capabilities.

## Code Structure
The code defines a `TestRAG` class that inherits from `BaseExecutorTest`. It contains multiple test methods that create models, execute SQL queries, and verify the results or expected errors.

## Symbols

### TestRAG
#### Description
A test class for RAG functionality, containing various test methods to validate different aspects of the RAG implementation.

#### Internal Logic
The class uses SQL queries to create and interact with models, and includes helper methods for waiting for predictor completion and running SQL commands.

### wait_predictor
#### Description
Waits for a predictor to complete or reach an error state.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| project | str | The project name |
| name | str | The predictor name |

#### Internal Logic
Polls the status of the predictor every 0.5 seconds for up to 100 seconds (200 attempts). Raises a RuntimeError if the predictor doesn't complete within this time.

### run_sql
#### Description
Executes a SQL command and returns the result as a DataFrame.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| sql | str | The SQL query to execute |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result | DataFrame | The query result as a pandas DataFrame |

### test_missing_required_keys
#### Description
Tests the behavior when required keys are missing in the model creation.

#### Internal Logic
Creates a model without specifying required parameters and expects an exception to be raised.

### test_invalid_model_id_parameter
#### Description
Tests the behavior when an invalid model ID is provided for OpenAI and Writer LLMs.

#### Internal Logic
Creates models with non-existent model IDs and expects exceptions to be raised.

### test_unsupported_llm_type
#### Description
Tests the behavior when an unsupported LLM type is specified.

#### Internal Logic
Creates a model with an unsupported LLM type and expects an exception to be raised.

### test_unsupported_vector_store
#### Description
Tests the behavior when an unsupported vector store is specified.

#### Internal Logic
Creates a model with an unsupported vector store and expects an exception to be raised.

### test_unknown_arguments
#### Description
Tests the behavior when unknown arguments are provided in the model creation.

#### Internal Logic
Creates a model with an invalid argument and expects an exception to be raised.

### test_qa
#### Description
Tests the question-answering capabilities of the RAG system using different configurations.

#### Internal Logic
1. Creates a sample dataset with context and URLs.
2. Tests OpenAI QA with ChromaDB.
3. Tests batching with OpenAI QA and ChromaDB.
4. Tests Writer QA with FAISS.
5. Tests single URL parsing.
6. Tests multi-URL parsing.

For each test, it creates a model, waits for completion, and verifies the answer to a test question.

### test_invalid_prompt_template
#### Description
Tests the behavior when an invalid prompt template is provided.

#### Internal Logic
Creates a model with an invalid prompt template format and expects an exception to be raised.

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| os | Accessing environment variables |
| time | Implementing delays in the wait_predictor method |
| pandas | Data manipulation and analysis |
| pytest | Testing framework |
| mindsdb_sql | SQL parsing |

## Configuration
The test suite uses environment variables for API keys:
| Option | Type | Default | Description |
|:-------|:-----|:--------|:------------|
| OPENAI_API_KEY | str | None | API key for OpenAI |
| WRITER_API_KEY | str | None | API key for Writer |
| WRITER_ORG_ID | str | None | Organization ID for Writer |

## Error Handling
The test suite uses pytest's exception handling to check for expected errors in various scenarios, such as missing required keys, invalid model IDs, and unsupported configurations.

## TODOs
There are no explicit TODOs in the code.