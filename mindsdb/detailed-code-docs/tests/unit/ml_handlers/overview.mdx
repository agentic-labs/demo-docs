---
title: "Overview"
---

## High-level description
This directory contains unit tests for various machine learning handlers and utilities used in MindsDB. The tests cover a wide range of ML models, APIs, and functionalities, including popular libraries and services like OpenAI, Hugging Face, LangChain, and custom implementations.

## What does it do?
The tests in this directory verify the correct behavior of different ML handlers and utilities. They ensure that:

1. Models can be created, trained, and used for predictions correctly.
2. API integrations work as expected, handling authentication and data exchange properly.
3. Error cases are handled appropriately, with meaningful error messages.
4. Various ML tasks such as classification, regression, time series forecasting, and natural language processing are functioning correctly.
5. The handlers integrate properly with MindsDB's SQL interface and data structures.

These tests help maintain the reliability and correctness of MindsDB's machine learning capabilities across different integrations and use cases.

## Key Files

### test_autogluon.py, test_huggingface.py, test_langchain.py, test_lightfm.py, test_lightwood.py, test_ludwig.py
These files test various open-source machine learning libraries integrated into MindsDB. They verify that models from these libraries can be created, trained, and used for predictions within the MindsDB ecosystem.

### test_openai.py, test_anthropic.py, test_anyscale_endpoints.py, test_clipdrop.py, test_leonardo_ai.py, test_stabilityai.py
These files test integrations with various AI APIs and services. They ensure that MindsDB can correctly authenticate, send requests, and process responses from these external services.

### test_merlion_handler.py, test_neuralforecast.py, test_statsforecast.py
These files focus on testing time series forecasting capabilities, including anomaly detection and hierarchical forecasting.

### test_rag.py, test_spacy.py
These files test natural language processing capabilities, including Retrieval-Augmented Generation (RAG) and various linguistic features provided by Spacy.

### test_time_series_utils.py
This file tests utility functions used in time series analysis and forecasting, such as frequency inference and data transformations.

### conftest.py
This file contains pytest fixtures and utility functions for loading test data, which are used across multiple test files in this directory.

## Dependencies
The tests in this directory rely on various external libraries and frameworks, including:

| Dependency | Purpose |
|:-----------|:--------|
| pytest | Testing framework |
| unittest.mock | Mocking objects for testing |
| pandas | Data manipulation and analysis |
| numpy | Numerical computations |
| mindsdb_sql | SQL parsing and execution within MindsDB |

Additionally, many tests require API keys or access tokens for external services, which are typically retrieved from environment variables.

## Configuration
Most tests use environment variables for configuration, particularly for API keys and access tokens. For example:

| Option | Type | Description |
|:-------|:-----|:------------|
| OPENAI_API_KEY | str | API key for OpenAI services |
| ANTHROPIC_API_KEY | str | API key for Anthropic services |
| STABILITY_API_KEY | str | API key for Stability AI services |

Tests that require these keys are often skipped if the corresponding environment variable is not set.

## Error Handling
The tests make extensive use of pytest's exception handling capabilities to check for expected errors in various scenarios. This includes testing for:

- Missing required parameters
- Invalid configurations
- Unsupported model types or operations
- API authentication failures

When errors are expected, the tests use `pytest.raises` to ensure that the correct exceptions are raised with appropriate error messages.