---
title: "test_anomaly_detection.py"
---

## High-level description
This file contains unit tests for the anomaly detection handler in MindsDB. It tests various aspects of the anomaly detection functionality, including model selection, data preprocessing, and different types of anomaly detection models (supervised, semi-supervised, and unsupervised).

## Code Structure
The file defines a main test class `TestAnomalyDetectionHandler` that inherits from `BaseExecutorTest`. This class contains multiple test methods, each focusing on a specific aspect of the anomaly detection functionality. The tests use mock handlers and SQL queries to simulate the creation and usage of anomaly detection models.

## Symbols

### `test_choose_model`
#### Description
This function tests the `choose_model` function from the anomaly detection handler, verifying that the correct model is selected based on different input parameters.

#### Internal Logic
- Tests various scenarios for model selection, including unsupervised, semi-supervised, and supervised models.
- Checks if the correct model is chosen based on the presence or absence of a target variable and dataset size.
- Verifies that specifying a model type or name overrides the default selection logic.
- Ensures that an error is raised when an invalid model name is provided.

### `test_preprocess_data`
#### Description
This function tests the `preprocess_data` function from the anomaly detection handler.

#### Internal Logic
- Loads a sample dataset.
- Calls the `preprocess_data` function.
- Asserts that the preprocessed dataframe has the same length as the original dataframe.

### `TestAnomalyDetectionHandler` class
#### Description
This class contains multiple test methods for the anomaly detection handler functionality.

#### Internal Logic
- Sets up the test environment using `BaseExecutorTest`.
- Implements a `wait_predictor` method to wait for predictor creation.
- Uses SQL queries to create and interact with anomaly detection models.
- Tests various scenarios including default supervised, non-default supervised, semi-supervised, and unsupervised models.
- Verifies the creation of ensemble models.

### `test_default_supervised_model`
#### Description
Tests the creation and usage of a default supervised anomaly detection model.

#### Internal Logic
- Creates a project and loads sample data.
- Creates a supervised model using SQL.
- Verifies that the model is created successfully.
- Runs predictions using the created model.
- Checks that the model type is "CatBoostClassifier".

### `test_non_default_supervised_model`
#### Description
Tests the creation and usage of a non-default supervised anomaly detection model.

#### Internal Logic
- Similar to `test_default_supervised_model`, but specifies a different model name ("nb" for Naive Bayes).
- Verifies that the correct model ("GaussianNB") is used.

### `test_specify_anomaly_type`
#### Description
Tests the creation of an unsupervised model with a specified anomaly type.

#### Internal Logic
- Creates an unsupervised model with the "clustered" anomaly type.
- Verifies that the correct model ("PCA") is used based on the specified anomaly type.

### `test_ensemble`
#### Description
Tests the creation and usage of an ensemble of anomaly detection models.

#### Internal Logic
- Creates an ensemble model using multiple specified models.
- Verifies that all specified models are created and used in the ensemble.

### `test_default_semi_supervised_model`
#### Description
Tests the creation and usage of a default semi-supervised anomaly detection model.

#### Internal Logic
- Creates a semi-supervised model.
- Verifies that the correct model ("XGBOD") is used.

### `test_default_unsupervised_model`
#### Description
Tests the creation and usage of a default unsupervised anomaly detection model.

#### Internal Logic
- Creates an unsupervised model without specifying a target column.
- Verifies that the correct model ("ECOD") is used.

## Dependencies
The test file depends on various MindsDB modules and external libraries such as pandas, unittest.mock, and mindsdb_sql.

## Error Handling
The tests use assertions to verify expected behaviors and results. If any assertion fails, it will raise an AssertionError, indicating a test failure.

## Performance Considerations
The tests include a `wait_predictor` method with a timeout to ensure that long-running model creation processes do not cause the tests to hang indefinitely.