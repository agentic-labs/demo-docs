---
title: "config.py"
---

## High-level description
This file defines configuration classes for various Language Model (LLM) providers using Pydantic. It includes configurations for OpenAI, Anthropic, Anyscale, LiteLLM, Ollama, NVIDIA NIM, and MindsDB. These classes are designed to store and validate configuration parameters for different LLM services.

## Code Structure
The file defines a base configuration class `BaseLLMConfig` and several provider-specific configuration classes that inherit from it. Each provider-specific class contains fields relevant to that particular LLM service.

## Symbols

### `BaseLLMConfig`
#### Description
A base Pydantic model for LLM configurations. It removes the 'model_' prefix from protected namespaces to avoid conflicts with Langchain constructor kwargs.

### `OpenAIConfig`
#### Description
Configuration class for OpenAI's LLM service.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| model_name | str | The name of the OpenAI model to use |
| temperature | Optional[float] | Controls randomness in output generation |
| max_retries | Optional[int] | Maximum number of retry attempts |
| max_tokens | Optional[int] | Maximum number of tokens to generate |
| openai_api_base | Optional[str] | Custom API base URL |
| openai_api_key | Optional[str] | API key for authentication |
| openai_organization | Optional[str] | Organization ID for API requests |
| request_timeout | Optional[float] | Timeout for API requests |

### `AnthropicConfig`
#### Description
Configuration class for Anthropic's LLM service.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| model | str | The name of the Anthropic model to use |
| temperature | Optional[float] | Controls randomness in output generation |
| max_tokens | Optional[int] | Maximum number of tokens to generate |
| top_p | Optional[float] | Controls diversity of output |
| top_k | Optional[int] | Controls diversity of output |
| default_request_timeout | Optional[float] | Default timeout for API requests |
| anthropic_api_key | Optional[str] | API key for authentication |
| anthropic_api_url | Optional[str] | Custom API URL |

### `AnyscaleConfig`
#### Description
Configuration class for Anyscale's LLM service.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| model_name | str | The name of the Anyscale model to use |
| temperature | Optional[float] | Controls randomness in output generation |
| max_retries | Optional[int] | Maximum number of retry attempts |
| max_tokens | Optional[int] | Maximum number of tokens to generate |
| anyscale_api_base | Optional[str] | Custom API base URL |
| anyscale_api_key | Optional[str] | API key for authentication |
| anyscale_proxy | Optional[str] | Proxy configuration |
| request_timeout | Optional[float] | Timeout for API requests |

### `LiteLLMConfig`
#### Description
Configuration class for LiteLLM service.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| model | str | The name of the LiteLLM model to use |
| api_base | Optional[str] | Custom API base URL |
| max_retries | Optional[int] | Maximum number of retry attempts |
| max_tokens | Optional[int] | Maximum number of tokens to generate |
| top_p | Optional[float] | Controls diversity of output |
| top_k | Optional[int] | Controls diversity of output |
| temperature | Optional[float] | Controls randomness in output generation |
| custom_llm_provider | Optional[str] | Custom LLM provider name |
| llm_model_kwargs | Optional[Dict[str, Any]] | Additional model-specific arguments |

### `OllamaConfig`
#### Description
Configuration class for Ollama's LLM service.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| base_url | str | Base URL for Ollama API |
| model | str | The name of the Ollama model to use |
| temperature | Optional[float] | Controls randomness in output generation |
| top_p | Optional[float] | Controls diversity of output |
| top_k | Optional[int] | Controls diversity of output |
| timeout | Optional[int] | Timeout for API requests |
| format | Optional[str] | Output format |
| headers | Optional[Dict] | Custom headers for API requests |
| num_predict | Optional[int] | Number of tokens to predict |
| num_ctx | Optional[int] | Context window size |
| num_gpu | Optional[int] | Number of GPUs to use |
| repeat_penalty | Optional[float] | Penalty for repeated tokens |
| stop | Optional[List[str]] | Stop sequences |
| template | Optional[str] | Custom prompt template |

### `NvidiaNIMConfig`
#### Description
Configuration class for NVIDIA NIM's LLM service.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| base_url | str | Base URL for NVIDIA NIM API |
| model | str | The name of the NVIDIA NIM model to use |
| temperature | Optional[float] | Controls randomness in output generation |
| top_p | Optional[float] | Controls diversity of output |
| timeout | Optional[int] | Timeout for API requests |
| format | Optional[str] | Output format |
| headers | Optional[Dict] | Custom headers for API requests |
| num_predict | Optional[int] | Number of tokens to predict |
| num_ctx | Optional[int] | Context window size |
| num_gpu | Optional[int] | Number of GPUs to use |
| repeat_penalty | Optional[float] | Penalty for repeated tokens |
| stop | Optional[List[str]] | Stop sequences |
| template | Optional[str] | Custom prompt template |
| nvidia_api_key | Optional[str] | API key for authentication |

### `MindsdbConfig`
#### Description
Configuration class for MindsDB's LLM service.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| model_name | str | The name of the MindsDB model to use |
| project_name | str | The name of the MindsDB project |

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| typing | Provides type hinting support |
| pydantic | Used for data validation and settings management |

This file serves as a central configuration hub for various LLM providers, allowing for easy management and validation of settings across different services within the MindsDB ecosystem.