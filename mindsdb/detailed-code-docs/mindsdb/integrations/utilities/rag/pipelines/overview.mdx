---
title: "Overview"
---

## High-level description

This directory contains the implementation of Retrieval-Augmented Generation (RAG) pipelines using LangChain's LCEL (LangChain Expression Language) components. The main functionality is encapsulated in the `LangChainRAGPipeline` class, which provides methods to create RAG pipelines with different types of retrievers, including simple vector store retrievers, auto retrievers, and multi-vector retrievers.

## What does it do?

The code in this directory enables the creation of sophisticated question-answering systems that combine the power of large language models with information retrieval from external sources. Here's a simplified explanation of the workflow:

1. When a user asks a question, the system first retrieves relevant information from a knowledge base using one of several retrieval methods.
2. The retrieved information is then combined with the original question to create a more informed prompt for the language model.
3. The language model generates an answer based on this enhanced prompt.
4. Optionally, the system can also return the sources of information used to generate the answer.

This approach allows the system to provide more accurate and contextually relevant answers by grounding the language model's responses in specific, retrieved information.

## Key Files

### rag.py

This file contains the core implementation of the RAG pipeline. The main components include:

1. `LangChainRAGPipeline` class: This is the central class that builds and configures the RAG pipeline. It provides methods to create pipelines with different types of retrievers and options for returning sources.

2. `with_returned_sources()` method: This method builds a RAG pipeline that not only generates an answer but also returns the sources used to create that answer.

3. Class methods for different retriever types:
   - `from_retriever()`: Creates a pipeline using a simple vector store retriever.
   - `from_auto_retriever()`: Creates a pipeline using an AutoRetriever.
   - `from_multi_vector_retriever()`: Creates a pipeline using a MultiVectorRetriever.

These methods allow for flexibility in how information is retrieved, catering to different use cases and data structures.

## Dependencies

The code relies heavily on the LangChain library, particularly its core components for building language model workflows. Key dependencies include:

1. `langchain_core`: Provides the foundational components for building the RAG pipeline, including runnables, prompts, and output parsers.

2. `mindsdb.integrations.utilities.rag`: Contains custom retrievers and utilities specific to the MindsDB project, such as `VectorStoreOperator`, `AutoRetriever`, and `MultiVectorRetriever`.

3. `BaseLLM`: An abstract base class for language models, allowing the pipeline to work with different LLM implementations.

The code is designed to be flexible and can work with various LLM implementations and retriever types, making it adaptable to different scenarios and requirements within the MindsDB ecosystem.

## Configuration

The RAG pipeline is configured using a `RAGPipelineModel`, which likely contains settings such as:

- The type of retriever to use
- The prompt template for the RAG system
- Configuration for the vector store or other retrieval mechanisms
- Settings for the language model

The exact structure of this configuration model is not provided in the given code snippet, but it's used consistently across the different pipeline creation methods.

In summary, this directory provides a powerful and flexible framework for building RAG pipelines, allowing MindsDB users to create sophisticated question-answering systems that leverage both large language models and efficient information retrieval techniques.