---
title: "finetune.py"
---

## High-level description
This code defines several functions for fine-tuning different types of Hugging Face Transformer models, including text classification, translation, and summarization. It leverages the `datasets` and `transformers` libraries for data handling and model training.

## Code Structure
The code consists of several functions, each responsible for fine-tuning a specific type of model. These functions share a common structure: they prepare the data, load the pre-trained model and tokenizer, define training arguments, and create a `Trainer` object to fine-tune the model. The `FINETUNE_MAP` dictionary in `settings.py` maps task types to their corresponding fine-tuning functions.

## References
This code references the `evaluate` and `nltk` libraries for metric computation and text processing, respectively. It also heavily relies on the `datasets` and `transformers` libraries for data handling and model training.

## Symbols

### `_finetune_cls`
#### Description
Fine-tunes a Hugging Face Transformer model for text classification.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| df | pandas.DataFrame | DataFrame containing the text data and labels. |
| args | dict | Dictionary containing the model name, target column, input column, labels map, and other training arguments. |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| tokenizer | transformers.PreTrainedTokenizer | The tokenizer used for the model. |
| trainer | transformers.Trainer | The trainer object used for fine-tuning. |

#### Internal Logic
1. Renames the DataFrame columns to 'text' and 'labels'.
2. Loads the pre-trained tokenizer and tokenizes the text data.
3. Splits the data into training and evaluation sets.
4. Loads the pre-trained model for sequence classification.
5. Defines the training arguments and metric for evaluation.
6. Creates a `Trainer` object and returns the tokenizer and trainer.

### `_finetune_translate`
#### Description
Fine-tunes a Hugging Face Transformer model for translation tasks.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| df | pandas.DataFrame | DataFrame containing the source and target language texts. |
| args | dict | Dictionary containing the model name, input/target language, and other training arguments. |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| tokenizer | transformers.PreTrainedTokenizer | The tokenizer used for the model. |
| trainer | transformers.Trainer | The trainer object used for fine-tuning. |

#### Internal Logic
1. Renames the DataFrame columns to 'text' and 'translation'.
2. Loads the pre-trained tokenizer and tokenizes the text data.
3. Splits the data into training and evaluation sets.
4. Loads the pre-trained model for sequence-to-sequence language modeling.
5. Defines the training arguments and data collator.
6. Creates a `Trainer` object and returns the tokenizer and trainer.

### `_finetune_summarization`
#### Description
Fine-tunes a Hugging Face Transformer model for summarization tasks.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| df | pandas.DataFrame | DataFrame containing the text data and summaries. |
| args | dict | Dictionary containing the model name, target column, input column, and other training arguments. |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| tokenizer | transformers.PreTrainedTokenizer | The tokenizer used for the model. |
| trainer | transformers.Trainer | The trainer object used for fine-tuning. |

#### Internal Logic
1. Renames the DataFrame columns to 'text' and 'summary'.
2. Loads the pre-trained tokenizer and tokenizes the text data.
3. Splits the data into training and evaluation sets.
4. Loads the pre-trained model for sequence-to-sequence language modeling.
5. Defines the training arguments, data collator, and ROUGE metric for evaluation.
6. Creates a `Trainer` object and returns the tokenizer and trainer.

### `_finetune_fill_mask`, `_finetune_text_generation`, `_finetune_question_answering`, `_finetune_text_2_text_generation`
#### Description
These functions are placeholders for future implementations of fine-tuning for other tasks like fill-mask, text generation, question answering, and text-to-text generation. Currently, they raise a `NotImplementedError`.

## Dependencies
This code depends on the following external libraries:
| Dependency | Purpose |
|:-----------|:--------|
| datasets | Handling and processing datasets. |
| transformers | Loading and fine-tuning Hugging Face Transformer models. |
| evaluate | Loading metrics for model evaluation. |
| nltk | Natural Language Toolkit for text processing. |
| numpy | Numerical operations. |

## TODOs
- Add support for question answering task.
- Add support for fill mask.
- Add support for text_generation (causal language model).
- Add support for text_2_text generation.
