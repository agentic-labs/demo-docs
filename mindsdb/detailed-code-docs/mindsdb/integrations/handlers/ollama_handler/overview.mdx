---
title: "Overview"
---

## High-level description
This directory contains the implementation of the Ollama handler for MindsDB, which integrates Ollama, a tool for local deployment of large language models, into the MindsDB ecosystem. The handler allows users to create, manage, and use Ollama models within MindsDB, providing access to powerful language models and their capabilities.

## What does it do?
The Ollama handler enables MindsDB users to:
1. Create AI engines using Ollama models
2. Deploy and use various Ollama models, such as llama2, within MindsDB
3. Generate text completions using the deployed models
4. Generate embeddings for text inputs (with supported models)
5. Customize prompts and model parameters
6. Access model information and descriptions

This integration allows users to leverage Ollama's locally deployed language models while benefiting from MindsDB's data integration and management capabilities.

## Key Files
1. `ollama_handler.py`: This is the core file containing the `OllamaHandler` class, which implements the main functionality for interacting with Ollama models. It handles model creation, validation, prediction, and description.

2. `__init__.py`: This file sets up the Ollama integration for MindsDB, importing the `OllamaHandler` class and defining metadata for the integration.

3. `__about__.py`: This file contains metadata about the Ollama handler, including its version, description, and licensing information.

4. `README.md`: Provides documentation on how to set up and use the Ollama integration within MindsDB, including prerequisites, setup instructions, and usage examples.

## Dependencies
The Ollama handler relies on the following main dependencies:
- `requests`: For making HTTP requests to the Ollama API
- `pandas`: For data manipulation and storage
- `mindsdb.integrations.libs.base`: Provides the base class for ML engines
- `mindsdb.integrations.libs.llm.utils`: Offers utility functions for LLM operations

## Configuration
The Ollama handler uses the following configuration options:
- `DEFAULT_SERVE_URL`: Set to "http://localhost:11434" by default, this is the URL for the Ollama service.
- `model_name`: Specifies the Ollama model to use (e.g., "llama2")
- `ollama_serve_url`: Allows customization of the Ollama service URL
- `mode`: Can be set to 'generate' or 'embedding' for supported models

Users can configure these options when creating an AI engine or a model using the Ollama handler.

## Example Usage
Here's a brief example of how to use the Ollama handler in MindsDB:

1. Create an AI engine:
```sql
CREATE ML_ENGINE ollama_engine
FROM ollama;
```

2. Create a model using the Ollama engine:
```sql
CREATE MODEL llama2_model
PREDICT completion
USING
   engine = 'ollama_engine',
   model_name = 'llama2';
```

3. Query the model for predictions:
```sql
SELECT text, completion
FROM llama2_model
WHERE text = 'Hello';
```

This integration provides a powerful way to incorporate locally deployed large language models into MindsDB workflows, enabling advanced natural language processing capabilities within the platform.