---
title: "Overview"
---

## High-level description
This directory contains the implementation of a Retrieval Augmented Generation (RAG) handler for MindsDB. The RAG handler integrates large language models, vector databases, and embedding models to create, train, and deploy models for semantic search and question-answering tasks within MindsDB.

## What does it do?
The RAG handler allows users to:
1. Create RAG models using various data sources (URLs, databases, files)
2. Ingest and process data, converting it into embeddings stored in vector databases
3. Perform question-answering tasks using the created models
4. Update existing models with new parameters
5. Integrate with different LLMs (e.g., OpenAI, Writer) and vector stores (e.g., ChromaDB, FAISS)

The workflow typically involves:
1. Creating a RAG engine with necessary API keys
2. Creating a RAG model by specifying data source, LLM type, and other parameters
3. Using the model to answer questions based on the ingested data

## Entry points
The main entry point for the RAG handler is the `RAGHandler` class in `rag_handler.py`. This class inherits from `BaseMLEngine` and implements the core functionality for creating, updating, and using RAG models within MindsDB.

Key files and their roles:
1. `rag_handler.py`: Implements the main `RAGHandler` class
2. `ingest.py`: Contains the `RAGIngestor` class for processing and embedding input data
3. `rag.py`: Implements the `RAGQuestionAnswerer` class for performing question-answering tasks
4. `settings.py`: Defines configuration classes, utility functions, and constants used throughout the handler

The data flow in the RAG handler is as follows:
1. Input data (from URLs, databases, or files) is processed by the `RAGIngestor`
2. Processed data is embedded and stored in a vector database
3. User queries are processed by the `RAGQuestionAnswerer`, which retrieves relevant context from the vector store and generates answers using the specified LLM

## Key Files
1. `__init__.py`: Sets up the RAG handler integration, defining metadata such as name, type, and version
2. `__about__.py`: Contains package metadata and version information
3. `exceptions.py`: Defines custom exception classes for error handling
4. `ingest.py`: Implements data ingestion and processing for the RAG system
5. `rag.py`: Contains the core question-answering functionality
6. `rag_handler.py`: Implements the main `RAGHandler` class for MindsDB integration
7. `settings.py`: Defines configuration classes, constants, and utility functions

## Dependencies
The RAG handler relies on several external libraries and frameworks:
- langchain: For various NLP components and vector store operations
- openai: For OpenAI API integration
- pandas: For data manipulation and analysis
- pydantic: For data validation and settings management
- chromadb: For vector database functionality
- sentence_transformers: For embedding models

## Configuration
The RAG handler uses various configuration options defined in `settings.py`, including:
- Default prompt templates
- Chunk sizes and overlap for text splitting
- Vector store settings
- LLM parameters

Users can configure the RAG handler by providing parameters when creating or updating models, such as:
- LLM type (e.g., OpenAI, Writer)
- Vector store type
- Embedding model name
- Input data source (URL, database, file)

The `RAGHandlerParameters` class in `settings.py` defines the full set of configurable parameters for the RAG handler.

In conclusion, the RAG handler provides a flexible and powerful integration for semantic search and question-answering tasks within MindsDB, allowing users to leverage state-of-the-art language models and vector databases for various applications.