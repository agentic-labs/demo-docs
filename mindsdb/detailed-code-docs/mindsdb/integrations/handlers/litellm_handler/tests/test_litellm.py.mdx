---
title: "test_litellm.py"
---

## High-level description
This file contains unit tests for the LiteLLM handler in MindsDB. It tests various scenarios of creating and using language models with the LiteLLM integration, including completion tasks with and without prompt templates, using OpenAI's GPT-3.5-turbo model.

## Code Structure
The main class `TestLiteLLM` inherits from `BaseExecutorTest`. It contains helper methods for running SQL commands and waiting for predictors to be created, as well as several test methods that create models and run predictions using different configurations.

## Symbols

### `TestLiteLLM`
#### Description
A test class that inherits from `BaseExecutorTest` and contains methods to test the LiteLLM handler functionality.

#### Internal Logic
The class sets up the testing environment, creates models, and runs predictions using SQL commands. It tests various scenarios such as completions without prompt templates, with prompt templates, and using messages for input.

### `wait_predictor`
#### Description
A helper method that waits for a predictor to be created and checks its status.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| project | str | The project name |
| name | str | The predictor name |

#### Internal Logic
Polls the database for the predictor's status, waiting for it to be "complete" or raising an error if it fails.

### `run_sql`
#### Description
A helper method that executes SQL commands and returns the result as a DataFrame.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| sql | str | The SQL command to execute |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result | DataFrame | The result of the SQL command |

### `test_completion_without_prompt_template`
#### Description
Tests completion tasks without using a prompt template.

#### Internal Logic
1. Creates a project and a model using the LiteLLM handler.
2. Tests completion with a simple text input.
3. Tests completion using messages input.
4. Tests batch completion with multiple messages.
5. Checks for expected failure when using incompatible arguments.

### `test_completion_openai_with_prompt_template`
#### Description
Tests completion tasks using prompt templates.

#### Internal Logic
1. Tests completion with a single format variable in the prompt template.
2. Tests completion with multiple format variables in the prompt template.
3. Tests using a prompt template in the predict statement.

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| pytest | Testing framework |
| mindsdb_sql | SQL parsing |
| os | Environment variable handling |
| time | Timing operations |

## Configuration
The test class uses environment variables for configuration:
| Option | Type | Default | Description |
|:-------|:-----|:--------|:------------|
| OPENAI_API_KEY | str | None | API key for OpenAI services |

## TODOs
- The `mock_response` parameter is used in several test cases, but its purpose and implementation are not clear from this file alone. It might be worth adding a comment or explanation about how this mocking works.
- Consider adding more specific assertions to verify the content of the responses, not just their existence.
- The error handling in the `wait_predictor` method could be improved to provide more detailed information about failures.

This test file provides comprehensive coverage of the LiteLLM handler's functionality within MindsDB, testing various scenarios of model creation and prediction using both simple inputs and more complex configurations with prompt templates.