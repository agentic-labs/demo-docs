---
title: "Overview"
---

## High-level description
This directory contains integration tests for the Minds Endpoint handler in MindsDB. The tests are designed to verify the functionality of the Minds Endpoint engine, including model creation, question answering, and embedding generation using the Minds Endpoint API.

## What does it do?
The tests in this directory ensure that the Minds Endpoint integration in MindsDB works correctly. They cover various scenarios such as:

1. Creating models with valid and invalid parameters
2. Performing single and bulk question answering tasks
3. Generating embeddings for text inputs

These tests help maintain the reliability and correctness of the Minds Endpoint integration by simulating real-world usage scenarios and verifying the expected outcomes.

## Key Files
The main file in this directory is `test_minds_endpoint_handler.py`, which contains the `TestMindsEndpoint` class. This class inherits from `BaseMLAPITest` and includes several test methods:

1. `setup_method`: Sets up the test environment by creating a project and a Minds Endpoint engine.
2. `test_create_model_raises_exception_with_invalid_model_parameter`: Verifies that an exception is raised when creating a model with an invalid parameter.
3. `test_create_model_raises_exception_with_unknown_model_argument`: Checks if an exception is raised when creating a model with an unknown argument.
4. `test_select_runs_no_errors_on_chat_completion_question_answering_single`: Tests a single question answering task using chat completion.
5. `test_select_runs_no_errors_on_chat_completion_question_answering_bulk`: Tests bulk question answering using chat completion.
6. `test_select_runs_no_errors_on_embeddings_completion_single`: Tests embedding generation for a single input.

## Dependencies
The tests rely on the following external libraries and frameworks:

1. pytest (version not specified): Used as the testing framework.
2. pandas (version not specified): Used for data manipulation and analysis, particularly in creating mock DataFrames for bulk testing.
3. unittest.mock (part of Python standard library): Used for mocking objects in tests.

These dependencies were likely chosen for their widespread use in the Python testing ecosystem and their ability to handle complex data structures and mocking scenarios.

## Configuration
The tests use an environment variable for configuration:

- `MDB_TEST_MINDS_ENDPOINT_API_KEY`: This environment variable should contain the API key for the Minds Endpoint tests. It is crucial for authenticating with the Minds Endpoint API during the tests.

Example usage in the code:

```python
api_key = os.environ.get('MDB_TEST_MINDS_ENDPOINT_API_KEY')
```

This configuration allows for secure handling of API keys without hardcoding them in the test files.

To illustrate the structure and functionality of the tests, here's a simplified example of one of the test methods:

```python
def test_select_runs_no_errors_on_chat_completion_question_answering_single(self):
    model_name = 'test_model'
    self.create_model(model_name, model_type='question_answering')
    self.wait_predictor_ready(model_name)

    query = f"""
        SELECT answer
        FROM proj.{model_name}
        WHERE question = 'What is the capital of France?'
    """
    result = self.run_sql_query(query)
    
    self.assertIn('Paris', result['answer'][0])
```

This test method creates a model for question answering, waits for it to be ready, then runs a SQL query to get an answer for a single question. It then asserts that the answer contains the expected text ('Paris' in this case).

The tests in this directory play a crucial role in ensuring the reliability and correctness of the Minds Endpoint integration in MindsDB, covering various aspects of its functionality and error handling.