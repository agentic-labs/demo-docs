---
title: "Overview"
---

## High-level description
This directory contains integration tests for the Anyscale Endpoints AI engine in MindsDB. The tests are designed to verify the functionality of the Anyscale Endpoints handler, covering various scenarios such as model creation, sentiment analysis, and question answering using both single and bulk queries.

## What does it do?
The tests in this directory ensure that the Anyscale Endpoints integration in MindsDB works correctly. They simulate real-world usage scenarios by:

1. Setting up a test environment with a project and an Anyscale Endpoints engine.
2. Creating models for different AI tasks (sentiment analysis and question answering).
3. Executing queries to perform predictions using these models.
4. Verifying the results of the predictions.

The tests cover both positive scenarios (successful model creation and prediction) and negative scenarios (handling of invalid inputs). They also test the integration's ability to handle both single and bulk queries, simulating real-world use cases where users might want to analyze one piece of text or process multiple inputs at once.

## Key Files
The main file in this directory is `test_anyscale_endpoints_handler.py`. This file contains a test class `TestAnyscaleEndpoints` that inherits from `BaseMLAPITest`. The class includes several test methods, each focusing on a specific aspect of the Anyscale Endpoints integration:

1. `test_create_model_raises_exception_with_invalid_model_parameter`: Ensures that the system properly handles attempts to create a model with an invalid model name.

2. `test_create_model_raises_exception_with_invalid_operation_mode`: Verifies that the system correctly handles attempts to create a model with an invalid operation mode.

3. `test_select_runs_no_errors_on_completion_sentiment_analysis_single`: Tests sentiment analysis for a single input using completion mode.

4. `test_select_runs_no_errors_on_completion_sentiment_analysis_bulk`: Tests sentiment analysis for multiple inputs using completion mode.

5. `test_select_runs_no_errors_on_chat_completion_question_answering_single`: Tests question answering for a single input using chat completion mode.

6. `test_select_runs_no_errors_on_chat_completion_question_answering_bulk`: Tests question answering for multiple inputs using chat completion mode.

Each test method sets up the necessary environment, creates models, executes queries, and verifies the results using assertions.

## Dependencies
The tests rely on several external libraries and frameworks:

1. pytest (version not specified): Used for test execution and assertions. Pytest is a powerful testing framework that simplifies test writing and provides rich assertion introspection.

2. pandas (version not specified): Used for data manipulation, particularly in bulk testing scenarios. Pandas is chosen for its efficient handling of structured data and its integration with SQL-like operations.

3. unittest.mock (part of Python standard library): Used for mocking the PostgreSQL handler in bulk testing scenarios. Mocking allows the tests to simulate database interactions without requiring an actual database connection.

These dependencies are chosen to provide a robust testing environment that can simulate real-world scenarios while maintaining test isolation and repeatability.

## Configuration
The tests use environment variables for configuration, particularly for setting up the Anyscale Endpoints engine. The API key for the Anyscale Endpoints service is expected to be set in the environment variable `ANYSCALE_API_KEY`. This approach allows for secure handling of sensitive information and easy configuration across different environments.

In the `setup_method`, the tests create a database named "proj" and an ML engine named "anyscale_endpoints_engine" using the Anyscale Endpoints integration. The configuration for this engine includes:

```python
{
    'api_key': os.environ['ANYSCALE_API_KEY'],
    'model': 'meta-llama/Llama-2-7b-chat-hf'
}
```

This configuration specifies the API key and the default model to be used for the tests. The model "meta-llama/Llama-2-7b-chat-hf" is used as an example, but this could be changed to test different models available through Anyscale Endpoints.

It's worth noting that there's a commented-out test method `test_create_model_raises_exception_with_unknown_model_argument` which is intended to test the handling of unknown arguments during model creation. This test is currently disabled and should be uncommented and implemented once the handler is updated to handle unknown arguments properly.