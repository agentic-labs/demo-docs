---
title: "Overview"
---

## High-level description
This directory contains the implementation of a Web Crawler handler for MindsDB. It provides functionality to crawl websites, extract content, and process web data using SQL-like queries. The handler is designed to integrate seamlessly with MindsDB's ecosystem, allowing users to interact with web content as if it were a database table.

## What does it do?
The Web Crawler handler allows users to:
1. Crawl specified websites and extract content
2. Process PDF files accessible via URLs
3. Execute SQL-like queries on the crawled web data
4. Filter and limit the crawled results
5. Convert HTML content to readable text and Markdown format
6. Perform parallel processing of multiple URLs for improved performance

The handler provides a flexible interface for users to retrieve web content, making it easy to integrate web data into MindsDB's machine learning and data analysis workflows.

## Entry points
The main entry point for this handler is the `WebHandler` class defined in `web_handler.py`. This class initializes the handler and registers a `CrawlerTable` instance, which processes SQL-like queries for web crawling.

The data flow typically follows this path:
1. User creates a database using the Web handler
2. User executes a SQL SELECT query against the `crawler` table
3. The `CrawlerTable.select` method processes the query, extracting conditions and limits
4. The `get_all_websites` function is called to perform the actual web crawling
5. Results are returned as a pandas DataFrame

## Key Files
1. `web_handler.py`: Contains the main `WebHandler` and `CrawlerTable` classes, which handle query processing and integration with MindsDB.
2. `urlcrawl_helpers.py`: Provides utility functions for web crawling, content extraction, and data processing. This includes functions for converting PDFs to Markdown, fetching website links, and extracting readable text from HTML.
3. `__init__.py`: Sets up the handler, imports necessary components, and handles potential import errors.
4. `__about__.py`: Contains metadata and package information for the Web Crawler handler.

## Dependencies
The Web Crawler handler relies on several external libraries:
- pandas: For data manipulation and DataFrame creation
- mindsdb_sql: For SQL parsing and processing
- requests: For making HTTP requests
- BeautifulSoup: For HTML parsing
- html2text: For converting HTML to readable text
- fitz (PyMuPDF): For PDF processing
- concurrent.futures: For parallel processing of URLs

These dependencies were chosen to provide robust web crawling capabilities, efficient data processing, and integration with MindsDB's SQL-like query system.

## Configuration
The Web Crawler handler can be configured using the following options:
1. `web_crawling_allowed_sites`: A list of allowed domains for web crawling, specified in the `config.json` file. This enhances security by restricting the handler to crawl only from specified domains.

When executing queries, users can configure:
- `LIMIT` clause: Specifies the maximum number of pages to crawl (required)
- `WHERE` clause: Can be used to specify the URL(s) to crawl and apply filters
- `crawl_depth`: Controls how deep the crawler should traverse from each base URL

Example usage:
```sql
SELECT * 
FROM my_web.crawler 
WHERE url = 'docs.mindsdb.com' 
LIMIT 10;
```

This query would crawl up to 10 pages from the 'docs.mindsdb.com' domain.

The handler is designed to be flexible and can handle various scenarios, including crawling multiple websites, processing PDF content, and applying custom filters to the crawled data.