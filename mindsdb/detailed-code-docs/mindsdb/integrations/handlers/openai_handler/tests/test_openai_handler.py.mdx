---
title: "test_openai_handler.py"
---

## High-level description
This file contains integration tests for the OpenAI handler in MindsDB. It tests various functionalities of the OpenAI integration, including different modes of operation (default, embedding, image, conversational, and conversational-full), single and bulk predictions, and error handling.

## Code Structure
The code is organized as a single test class `TestOpenAI` that inherits from `BaseMLAPITest`. It contains multiple test methods, each testing a specific aspect of the OpenAI integration. The tests use SQL queries to create models, make predictions, and verify results.

## Symbols

### `TestOpenAI`
#### Description
A test class that contains integration tests for the OpenAI handler in MindsDB.

#### Internal Logic
1. Sets up the test environment by creating a project and an OpenAI engine.
2. Tests various scenarios:
   - Creating a model with an unsupported model name
   - Full flow in default mode with question column (single and bulk predictions)
   - Full flow in default mode with prompt template (single and bulk predictions)
   - Full flow in embedding mode (single and bulk predictions)
   - Full flow in image mode (single and bulk predictions)
   - Full flow in conversational mode (single and bulk predictions)
   - Full flow in conversational-full mode (single and bulk predictions)

### `setup_method`
#### Description
Sets up the test environment by creating a project and an OpenAI engine.

#### Internal Logic
1. Calls the superclass's `setup_method`.
2. Creates a database named "proj".
3. Creates an ML engine named "openai_engine" using the OpenAI API key.

### `test_create_model_with_unsupported_model_raises_exception`
#### Description
Tests if creating a model with an unsupported model name raises an exception.

#### Internal Logic
1. Attempts to create a model with an unsupported model name.
2. Waits for the predictor and expects an exception to be raised.
3. Asserts that the exception message contains "Invalid model name."

### `test_full_flow_in_default_mode_with_question_column_for_single_prediction_runs_no_errors`
#### Description
Tests the full flow in default mode with a question column for a single prediction.

#### Internal Logic
1. Creates a model using the OpenAI engine with a question column.
2. Waits for the predictor to be ready.
3. Runs a SQL query to make a prediction.
4. Asserts that the answer contains "stockholm".

### `test_full_flow_in_default_mode_with_question_column_for_bulk_predictions_runs_no_errors`
#### Description
Tests the full flow in default mode with a question column for bulk predictions.

#### Internal Logic
1. Creates a mock DataFrame with questions.
2. Creates a model using the OpenAI engine with a question column.
3. Waits for the predictor to be ready.
4. Runs a SQL query to make bulk predictions.
5. Asserts that the answers contain expected keywords.

(Similar patterns are followed for other test methods, each testing different modes and scenarios)

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| os | Accessing environment variables |
| pytest | Testing framework |
| pandas | Data manipulation |
| unittest.mock | Mocking objects for testing |
| tests.unit.ml_handlers.base_ml_test | Base class for ML API tests |

## Error Handling
The code uses pytest's `raises` context manager to test for exceptions in the `test_create_model_with_unsupported_model_raises_exception` method.

## TODOs
There is a commented-out test method `test_full_flow_finetune_runs_no_errors` with a TODO comment indicating that it needs to be fixed for fine-tuning functionality.