---
title: "Overview"
---

## High-level description
This directory contains integration tests for the OpenAI handler in MindsDB. The tests are designed to verify the functionality of various aspects of the OpenAI integration, including different modes of operation, single and bulk predictions, and error handling. The test suite is comprehensive and covers the main features of the OpenAI handler.

## What does it do?
The test suite in this directory performs the following tasks:

1. Sets up a test environment by creating a project and an OpenAI engine.
2. Tests the creation of models with different configurations, including handling of unsupported model names.
3. Verifies the functionality of various OpenAI modes:
   - Default mode with question column and prompt template
   - Embedding mode
   - Image mode
   - Conversational mode
   - Conversational-full mode
4. Checks both single and bulk predictions for each mode.
5. Ensures proper error handling when creating models with unsupported configurations.

These tests help ensure that the OpenAI integration in MindsDB works correctly across different scenarios and can handle various types of inputs and prediction requests.

## Key Files

### test_openai_handler.py
This is the main test file containing the `TestOpenAI` class, which inherits from `BaseMLAPITest`. It includes multiple test methods, each focusing on a specific aspect of the OpenAI integration:

1. `test_create_model_with_unsupported_model_raises_exception`: Verifies that creating a model with an unsupported name raises an appropriate exception.
2. `test_full_flow_in_default_mode_with_question_column_for_single_prediction_runs_no_errors`: Tests the default mode with a question column for single predictions.
3. `test_full_flow_in_default_mode_with_question_column_for_bulk_predictions_runs_no_errors`: Checks the default mode with a question column for bulk predictions.
4. Similar tests for default mode with prompt template, embedding mode, image mode, conversational mode, and conversational-full mode, each for both single and bulk predictions.

The file uses SQL queries to create models, make predictions, and verify results, simulating real-world usage of the OpenAI integration in MindsDB.

## Dependencies
The test suite relies on the following external libraries and frameworks:

1. pytest (version not specified): Used as the testing framework.
2. pandas (version not specified): Utilized for data manipulation, particularly in creating mock DataFrames for bulk prediction tests.
3. unittest.mock (Python standard library): Employed for mocking objects during testing.
4. tests.unit.ml_handlers.base_ml_test: Provides the `BaseMLAPITest` class, which serves as the base class for the `TestOpenAI` class.

These dependencies are chosen to provide a robust testing environment and to simulate various scenarios that the OpenAI handler might encounter in real-world usage.

## Configuration
The test suite uses environment variables for configuration, particularly for the OpenAI API key. This is accessed using the `os` module:

```python
os.environ['OPENAI_API_KEY']
```

This approach allows for secure handling of sensitive information and easy configuration across different testing environments.

Additionally, the tests create a project named "proj" and an ML engine named "openai_engine" using the OpenAI API key. This setup is performed in the `setup_method` of the `TestOpenAI` class:

```python
def setup_method(self):
    super().setup_method()
    self.run_sql("CREATE DATABASE proj")
    self.run_sql(
        f"""
        CREATE ML_ENGINE openai_engine
        FROM openai
        USING api_key = '{os.environ['OPENAI_API_KEY']}';
    """
    )
```

This configuration ensures that each test runs in a clean, isolated environment with the necessary OpenAI integration set up.

The test suite is comprehensive and well-structured, covering various aspects of the OpenAI integration in MindsDB. It provides a solid foundation for ensuring the reliability and correctness of the OpenAI handler across different modes of operation and prediction scenarios.