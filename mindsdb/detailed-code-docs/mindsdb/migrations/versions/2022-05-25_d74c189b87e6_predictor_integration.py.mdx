---
title: "2022-05-25_d74c189b87e6_predictor_integration.py"
---

## High-level description
This migration script, part of the MindsDB project, updates the database schema to integrate predictors with data sources. It adds integration-related columns to the predictor table, removes the dataset and analysis tables, and updates existing data to reflect these changes.

## Code Structure
The script defines two main functions: `upgrade()` and `downgrade()`. The `upgrade()` function modifies the database schema and migrates data, while the `downgrade()` function reverses these changes.

## Symbols

### `upgrade()`
#### Description
This function upgrades the database schema by adding new columns to the predictor table, inserting data into the integration table, updating existing predictor data, and removing obsolete tables and columns.

#### Internal Logic
1. Adds 'integration_id' and 'fetch_data_query' columns to the predictor table.
2. Inserts 'files' integration records for each distinct company.
3. Retrieves predictor data and updates it with integration information.
4. Removes obsolete columns and tables (dataset_id, analysis, dataset).

### `downgrade()`
#### Description
This function reverts the changes made by the `upgrade()` function, restoring the database schema to its previous state.

#### Internal Logic
1. Adds back the 'dataset_id' column to the predictor table.
2. Recreates the 'analysis' and 'dataset' tables.
3. Restores foreign key constraints and unique constraints.
4. Removes the 'integration_id' and 'fetch_data_query' columns from the predictor table.

## Side Effects
- Modifies the structure of the 'predictor' table.
- Removes the 'dataset' and 'analysis' tables.
- Updates existing data in the 'predictor' table.
- Inserts new records into the 'integration' table.

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| alembic | Database migration tool |
| sqlalchemy | SQL toolkit and ORM |
| json | JSON parsing for data manipulation |

## Error Handling
The script includes basic error handling for JSON parsing and data type checking but does not implement comprehensive error management.

## Performance Considerations
The script performs bulk operations on the database, which may impact performance for large datasets. It uses batch operations where possible to optimize performance.