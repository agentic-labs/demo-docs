---
title: "local-vs-2.1.7-canary.2.ts"
---

## High-level description
This code is a benchmark script that compares the performance of two versions of a React Email preview server: a local version and a canary version (2.1.7-canary.2). It sets up both servers, runs a series of fetch requests to each, measures the performance, and saves the results to a JSON file.

## Code Structure
The main functionality is wrapped in an immediately invoked async function. It uses the `Bench` class from `tinybench` to set up and run the benchmarks, and utilizes utility functions like `runServer` to start the preview servers.

## References
- `runServerAndFetchPreviewPage` (imported but not used in this file)
- `runServer` (from "./utils/run-server")

## Symbols

### Anonymous Async Function
#### Description
This is the main function that sets up and runs the benchmarks, comparing the performance of local and canary versions of the React Email preview server.

#### Internal Logic
1. Creates a new `Bench` instance with 30 iterations.
2. Starts both local and canary servers using `runServer`.
3. Adds two benchmark tests: one for the local server and one for the canary server.
4. Performs a warm-up fetch for both servers.
5. Runs the benchmark.
6. Kills both server subprocesses.
7. Writes the benchmark results to a JSON file.

### `bench` (Bench instance)
#### Description
An instance of the `Bench` class from `tinybench`, configured to run 30 iterations of the benchmark tests.

### `localServer` and `canaryServer`
#### Description
These are `Server` objects returned by the `runServer` function, representing the local and canary versions of the React Email preview server.

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| path | For resolving file paths |
| tinybench | For running benchmarks |
| node:fs | For writing results to a file |

## Error Handling
The code doesn't implement explicit error handling. Errors that occur during the execution of the async function will cause the promise to reject, potentially crashing the script.

## Performance Considerations
The script is designed to measure performance, running 30 iterations of fetch requests to each server. It includes a warm-up fetch for both servers before running the actual benchmark, which can help ensure more consistent results by allowing the servers to initialize fully.

## TODOs
- The `runServerAndFetchPreviewPage` function is imported but not used in this file. Consider removing it if it's not needed.
- Consider adding error handling to gracefully manage potential issues during server startup or benchmark execution.
- The benchmark results are saved with a fixed filename. Consider parameterizing this or including a timestamp to avoid overwriting previous results.