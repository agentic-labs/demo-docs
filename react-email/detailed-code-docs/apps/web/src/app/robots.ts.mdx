---
title: "robots.ts"
---

## High-level description
This code defines a function `Robots` that returns a configuration object for a `robots.txt` file. This file is used by web crawlers to understand which parts of a website they are allowed to access.

## Symbols

### Symbol Name: `Robots`
#### Description
This function returns a configuration object containing rules, sitemap location, and host for the `robots.txt` file.

#### Inputs
This function has no inputs.

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| return value | object | An object containing the following properties: &lt;br&gt; - `rules`: An array of rules defining which user agents can access which parts of the website. &lt;br&gt; - `sitemap`: The URL of the website's sitemap. &lt;br&gt; - `host`: The base URL of the website. |

#### Internal Logic
The function defines a single rule that allows all user agents (`userAgent: "*"`) to access the root path (`allow: "/"`). It also specifies the sitemap location and the website's host.

## Side Effects
This code has no side effects on its own. It's meant to be used by a static site generator to create the `robots.txt` file. 
