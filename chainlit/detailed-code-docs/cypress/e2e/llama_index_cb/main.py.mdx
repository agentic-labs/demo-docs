---
title: "main.py"
---

## High-level description
This code demonstrates the usage of the LlamaIndexCallbackHandler in a Chainlit application. It simulates a retrieval and LLM (Language Model) event sequence, showcasing how to use callbacks to track and display these events in a chat interface.

## Symbols

### `start()`
#### Description
This asynchronous function is triggered when a chat session starts. It demonstrates the usage of LlamaIndexCallbackHandler by simulating retrieval and LLM events.

#### Inputs
None

#### Outputs
None

#### Internal Logic
1. Sends an initial message "LlamaIndexCb" to the chat interface.
2. Creates an instance of `LlamaIndexCallbackHandler`.
3. Simulates a retrieval event:
   - Starts the event
   - Waits for 0.2 seconds
   - Ends the event with a simulated retrieved node
4. Simulates an LLM event:
   - Starts the event
   - Waits for 0.2 seconds
   - Ends the event with a simulated LLM response and prompt

#### Side Effects
- Sends messages to the Chainlit chat interface
- Triggers callback events for retrieval and LLM operations

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| llama_index.core.callbacks.schema | Provides event types and payload schemas for callbacks |
| llama_index.core.llms | Provides ChatMessage and ChatResponse classes for LLM interactions |
| llama_index.core.schema | Provides NodeWithScore and TextNode classes for representing retrieved information |
| chainlit | Provides the chat interface and callback handler |

## Error Handling
This code does not implement explicit error handling. It relies on the default error handling provided by the async runtime and the Chainlit framework.

## API/Interface Reference
| Decorator | Description |
|:----------|:------------|
| @cl.on_chat_start | Indicates that the `start()` function should be called when a new chat session begins |

Note: The code demonstrates the use of the LlamaIndexCallbackHandler API, but it's simulating the events rather than making actual calls to a LlamaIndex instance.