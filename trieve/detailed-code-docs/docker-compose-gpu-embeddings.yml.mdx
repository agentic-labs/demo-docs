---
title: "docker-compose-gpu-embeddings.yml"
---

## High-level description
This Docker Compose file defines a multi-container application for running various text embedding and reranking models using GPU acceleration. It sets up five separate services, each running a different model for text processing tasks such as document embedding, query embedding, and reranking.

## Code Structure
The file defines five services: `splade-doc`, `splade-query`, `jina`, `bgem3`, and `reranker`. Each service is configured to run a specific model using the same base Docker image but with different parameters and port mappings.

## Symbols

### `splade-doc` Service
#### Description
This service runs the SPLADE (Sparse Lexical and Expansion) model for document embedding.

#### Internal Logic
- Uses the `naver/efficient-splade-VI-BT-large-doc` model
- Exposes port 4000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration

### `splade-query` Service
#### Description
This service runs the SPLADE model optimized for query embedding.

#### Internal Logic
- Uses the `naver/efficient-splade-VI-BT-large-query` model
- Exposes port 5000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration

### `jina` Service
#### Description
This service runs the Jina AI embeddings model for text processing.

#### Internal Logic
- Uses the `jinaai/jina-embeddings-v2-base-en` model
- Exposes port 6000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration

### `bgem3` Service
#### Description
This service runs the BGE-M3 embeddings model from BAAI.

#### Internal Logic
- Uses the `BAAI/bge-m3` model
- Exposes port 7000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration

### `reranker` Service
#### Description
This service runs a reranker model, specifically the BGE reranker from BAAI.

#### Internal Logic
- Uses the `BAAI/bge-reranker-large` model
- Exposes port 8000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration
- Uses a specific revision (`refs/pr/4`) which might indicate a pre-release or development version

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| Docker | Container runtime for running the services |
| NVIDIA GPU | Required for GPU acceleration of the models |
| HuggingFace's text-embeddings-inference | Base image for running the embedding models |

## Configuration
| Option | Type | Default | Description |
|:-------|:-----|:--------|:------------|
| `--model-id` | string | Varies | Specifies the HuggingFace model ID to be used |
| `--revision` | string | Varies | Specifies the model revision or branch to use |
| `--pooling` | string | "splade" (for SPLADE models) | Specifies the pooling method for the model |

## API/Interface Reference
| Endpoint | Method | Request | Response | Description |
|:---------|:-------|:--------|:---------|:------------|
| `:4000` | - | - | - | SPLADE document embedding service |
| `:5000` | - | - | - | SPLADE query embedding service |
| `:6000` | - | - | - | Jina AI embedding service |
| `:7000` | - | - | - | BGE-M3 embedding service |
| `:8000` | - | - | - | BGE reranker service |

Note: The specific API methods and request/response formats are not detailed in this Docker Compose file. They would depend on the implementation of the `text-embeddings-inference` image and the specific models being used.