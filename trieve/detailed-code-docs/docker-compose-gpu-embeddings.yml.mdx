---
title: "docker-compose-gpu-embeddings.yml"
---

## High-level description
This Docker Compose file defines a multi-container application for running various text embedding and reranking models using GPU acceleration. It sets up five separate services, each running a different model for text processing tasks such as document embedding, query embedding, and reranking.

## Code Structure
The file defines five services: `splade-doc`, `splade-query`, `jina`, `bgem3`, and `reranker`. Each service is configured to run a specific model using the same base Docker image but with different parameters and port mappings.

## Symbols

### `splade-doc` service
#### Description
This service runs the SPLADE (Sparse Lexical and Expansion) model for document embedding.

#### Internal Logic
- Uses the `naver/efficient-splade-VI-BT-large-doc` model
- Exposes port 4000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration

### `splade-query` service
#### Description
This service runs the SPLADE model optimized for query embedding.

#### Internal Logic
- Uses the `naver/efficient-splade-VI-BT-large-query` model
- Exposes port 5000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration

### `jina` service
#### Description
This service runs the Jina AI embeddings model for text processing.

#### Internal Logic
- Uses the `jinaai/jina-embeddings-v2-base-en` model
- Exposes port 6000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration

### `bgem3` service
#### Description
This service runs the BGE-M3 embeddings model from BAAI.

#### Internal Logic
- Uses the `BAAI/bge-m3` model
- Exposes port 7000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration

### `reranker` service
#### Description
This service runs a reranker model, specifically the BGE reranker from BAAI.

#### Internal Logic
- Uses the `BAAI/bge-reranker-large` model
- Exposes port 8000 for external access
- Mounts a local `./data` directory to `/data` in the container
- Utilizes NVIDIA GPU acceleration
- Uses a specific revision (`refs/pr/4`) which might indicate a pre-release or development version

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| Docker | Container runtime for running the services |
| NVIDIA GPU | Required for GPU acceleration of the models |
| HuggingFace's text-embeddings-inference | Base image for running the embedding models |

## Configuration
| Option | Type | Default | Description |
|:-------|:-----|:--------|:------------|
| `--model-id` | string | varies | Specifies the HuggingFace model ID to be used for each service |
| `--revision` | string | varies | Specifies the model revision or branch to use |
| `--pooling` | string | splade (for SPLADE models) | Specifies the pooling method for the SPLADE models |

## API/Interface Reference
| Endpoint | Method | Request | Response | Description |
|:---------|:-------|:--------|:---------|:------------|
| localhost:4000 | POST | Text data | Embedded representation | SPLADE document embedding |
| localhost:5000 | POST | Text data | Embedded representation | SPLADE query embedding |
| localhost:6000 | POST | Text data | Embedded representation | Jina AI embedding |
| localhost:7000 | POST | Text data | Embedded representation | BGE-M3 embedding |
| localhost:8000 | POST | Text data | Reranked results | BGE reranker |

Note: The exact API endpoints and request/response formats would depend on the specific implementation of the `text-embeddings-inference` image and may require additional documentation for precise usage.