---
title: "Overview"
---

## High-level description
This directory contains the necessary files to build and run a Docker container for a Python script that optimizes search query analytics by collapsing similar queries in a ClickHouse database. The main components are a Dockerfile, a Python script (`collapse_queries.py`), and a README file explaining the purpose and functionality of the script.

## What does it do?
The script in this directory addresses the issue of redundant partial queries in search analytics data. It processes queries across multiple datasets stored in a ClickHouse database, identifying and removing partial queries that are prefixes of longer, more complete queries, but only if they occur within a 10-second window of each other. This optimization helps to reduce data redundancy and improve the accuracy of search analytics results.

The workflow is as follows:
1. Connect to the ClickHouse database
2. Retrieve a list of all datasets from the `search_queries` table
3. For each dataset:
   - Fetch the timestamp of the last collapse operation
   - Retrieve search queries in batches
   - Identify partial queries that are prefixes of longer queries within a 10-second window
   - Mark identified partial queries as duplicates in the database
   - Update the last processed timestamp for the dataset
4. Optimize the affected tables in the database

## Key Files

### Dockerfile
This file defines the environment for running the `collapse_queries.py` script. It sets up a lightweight Python 3.9 container, installs the required dependencies from a `requirements.txt` file, and configures the container to execute the script when launched.

Key aspects of the Dockerfile:
- Uses `python:3.9-slim` as the base image
- Sets `/app` as the working directory
- Copies `requirements.txt` and installs dependencies
- Copies `collapse_queries.py` into the container
- Sets the command to run the script on container start

### collapse_queries.py
This is the main Python script that performs the query collapse operation. It contains several functions to interact with the ClickHouse database, retrieve and process search queries, and update the database with the results.

Key functions in the script:
- `get_search_queries`: Retrieves search queries for a specific dataset
- `get_datasets`: Gets a list of all dataset IDs
- `get_dataset_last_collapsed` and `set_dataset_last_collapsed`: Manage the timestamp of the last collapse operation for each dataset
- `collapse_queries`: Identifies partial queries that should be marked as duplicates
- `insert_duplicate_rows`: Updates the database with duplicate information
- `main`: Orchestrates the entire process across all datasets

### README.md
This file provides an overview of the script's purpose, functionality, and main components. It explains the query collapse logic and lists the main functions used in the script.

## Dependencies
The script relies on the following external libraries:
- clickhouse_connect: For interacting with the ClickHouse database
- python-dotenv: For loading environment variables from a .env file

These dependencies are installed in the Docker container based on the `requirements.txt` file (not provided in the directory summary).

## Configuration
The script uses environment variables for configuration, which are likely loaded from a `.env` file (not present in the directory). The main configuration is the database connection string (DSN) for the ClickHouse database.

Key configurable aspects:
- ClickHouse database connection details
- Batch size for processing queries (default: 5000)
- Time window for considering partial queries (10 seconds)

In conclusion, this directory contains a containerized solution for optimizing search query data in a ClickHouse database by collapsing similar queries within a specific time window. The solution is designed to be run as a Docker container, making it easy to deploy and execute in various environments.