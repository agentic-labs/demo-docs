---
title: "file-worker.rs"
---

Here's a detailed documentation of the `file-worker.rs` file:

## High-level description
This file implements a worker service for processing file uploads in the Trieve server. It handles the asynchronous ingestion of files, breaking them into chunks, and storing them in the database and search engine.

## Code Structure
The main function sets up the server environment, including database connections, Redis connections, and analytics. The `file_worker` function is the core of the file processing logic, which runs in a loop to process file upload messages from a Redis queue.

## Symbols

### `main`
#### Description
The entry point of the program. It sets up the environment, initializes connections, and starts the file worker.

#### Internal Logic
1. Initializes environment variables and Sentry for error tracking.
2. Sets up database and Redis connections.
3. Configures analytics if enabled.
4. Starts the file worker service.

### `file_worker`
#### Description
The main worker function that processes file upload messages from a Redis queue.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| should_terminate | Arc&lt;AtomicBool&gt; | A flag to signal termination |
| redis_pool | web::Data&lt;RedisPool&gt; | Redis connection pool |
| web_pool | web::Data&lt;Pool&gt; | Database connection pool |
| event_queue | web::Data&lt;EventQueue&gt; | Queue for analytics events |

#### Internal Logic
1. Establishes a connection to Redis.
2. Enters a loop to process messages from the "file_ingestion" queue.
3. For each message, calls `upload_file` to process the file.
4. Handles errors and retries if necessary.

### `upload_file`
#### Description
Processes a single file upload message.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| file_worker_message | FileWorkerMessage | Message containing file upload details |
| web_pool | web::Data&lt;Pool&gt; | Database connection pool |
| event_queue | web::Data&lt;EventQueue&gt; | Queue for analytics events |
| redis_conn | MultiplexedConnection | Redis connection |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| Result&lt;Option&lt;uuid::Uuid&gt;, ServiceError&gt; | The ID of the uploaded file, or None if chunks were not created |

#### Internal Logic
1. Retrieves the file from S3.
2. Uses Apache Tika to convert the file to HTML.
3. Creates a database entry for the file.
4. If specified, creates chunks from the file content.
5. Sends analytics events for the file upload.

### `readd_error_to_queue`
#### Description
Requeues a failed file upload message for retry.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| payload | FileWorkerMessage | The original upload message |
| error | ServiceError | The error that occurred |
| event_queue | web::Data&lt;EventQueue&gt; | Queue for analytics events |
| redis_pool | web::Data&lt;RedisPool&gt; | Redis connection pool |

#### Internal Logic
1. Increments the attempt number for the message.
2. If the maximum attempts are reached, moves the message to a dead letter queue.
3. Otherwise, requeues the message for another attempt.

## Error Handling
The code uses a `ServiceError` enum for error handling. Errors are logged and, in some cases, sent to Sentry for monitoring. Failed uploads are retried up to 3 times before being moved to a dead letter queue.

## Logging
The code uses the `log` crate for logging information and errors throughout the file processing workflow.

## Dependencies
- `diesel_async`: For asynchronous database operations
- `redis`: For interacting with Redis queues
- `sentry`: For error tracking and monitoring
- `tracing_subscriber`: For logging and tracing
- `actix_web`: For web framework utilities
- `tokio`: For asynchronous runtime

## Configuration
The code relies on several environment variables for configuration, including database URLs, Redis settings, and feature flags like analytics usage.

This file is a crucial part of the Trieve server's file processing pipeline, handling the asynchronous ingestion and chunking of uploaded files.